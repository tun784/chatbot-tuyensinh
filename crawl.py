import requests
import unicodedata
from bs4 import BeautifulSoup
import re
import os
import string
headers = {
    "User-Agent": "Mozilla/5.0"
}
def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # M·∫∑t c∆∞·ªùi, c·∫£m x√∫c
        "\U0001F300-\U0001F5FF"  # Bi·ªÉu t∆∞·ª£ng kh√°c (m√¢y, th·ªùi ti·∫øt, v.v.)
        "\U0001F680-\U0001F6FF"  # Ph∆∞∆°ng ti·ªán giao th√¥ng, bi·ªÉu t∆∞·ª£ng b·∫£n ƒë·ªì
        "\U0001F700-\U0001F77F"  # Bi·ªÉu t∆∞·ª£ng alchemical
        "\U0001F780-\U0001F7FF"  # Bi·ªÉu t∆∞·ª£ng h√¨nh h·ªçc m·ªü r·ªông
        "\U0001F800-\U0001F8FF"  # Bi·ªÉu t∆∞·ª£ng m≈©i t√™n
        "\U0001F900-\U0001F9FF"  # Bi·ªÉu t∆∞·ª£ng b·ªï sung
        "\U0001FA00-\U0001FA6F"  # Bi·ªÉu t∆∞·ª£ng b·ªï sung kh√°c
        "\U0001FA70-\U0001FAFF"
        "\U00002700-\U000027BF"  # D·∫•u ƒë·∫∑c bi·ªát (bao g·ªìm üî∞)
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)
# H√†m lo·∫°i b·ªè ph·∫ßn footer kh√¥ng c·∫ßn thi·∫øt
def clean_html_footer(html):
    # C·∫Øt b·ªè ƒëo·∫°n cu·ªëi n·∫øu c√≥ c·ª•m n√†y
    parts = re.split(r"ƒê·ªÉ bi·∫øt th√™m th√¥ng tin tuy·ªÉn sinh", html, flags=re.IGNORECASE)
    return parts[0]

def remove_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFD', text)
        if unicodedata.category(c) != 'Mn'
    )

# H√†m tr√≠ch xu·∫•t M√£ ng√†nh t·ª´ HTML
def extract_ma_nganh(soup):
    candidates = soup.find_all(string=re.compile(r"M√£ ng√†nh[:\s]?", re.IGNORECASE))
    for txt in candidates:
        full = txt.strip()
        # Tr∆∞·ªùng h·ª£p m√£ n·∫±m c√πng text
        m = re.search(r"\d{6,}", full)
        if m:
            return m.group(0)
        # Tr∆∞·ªùng h·ª£p m√£ ·ªü th·∫ª k·∫ø ti·∫øp
        if txt.parent:
            sib = txt.parent.find_next_sibling(string=re.compile(r"\d{6,}"))
            if sib:
                m2 = re.search(r"\d{6,}", sib.strip())
                if m2:
                    return m2.group(0)
    return None

def crawl_page(url):
    print(f">>> ƒêang crawl: {url}")
    response = requests.get(url, headers=headers)
    response.encoding = "utf-8"  # ƒê·∫£m b·∫£o ƒë·ªçc ti·∫øng Vi·ªát
    raw_html = clean_html_footer(response.text)
    soup = BeautifulSoup(raw_html, "html.parser")

    ma_nganh = extract_ma_nganh(soup)

    output_lines = []

    title_tag = soup.find("h1")
    if title_tag:
        title = title_tag.get_text(strip=True)
        output_lines.append(f"{title}")

    # print(soup.prettify())

    # X·ª≠ l√Ω ph·∫ßn sau m√£ ng√†nh
    deepest_spans = []
    for p in soup.find_all('p'):
        if any(x in p.get_text() for x in ["M√É NG√ÄNH:", "M√£ ng√†nh:", "m√£ ng√†nh:"]):
            ma_nganh_p = p
            next_p = ma_nganh_p.find_next_sibling('p')
            if next_p:
                inner_spans = next_p.find_all('span')
                for span in inner_spans:
                    if not span.find('span'):
                        text = remove_emojis(span.get_text(strip=True))
                        if text:
                            deepest_spans.append(text)
            break

    if deepest_spans:
        output_lines.append(" ".join(deepest_spans))
    else:
        output_lines.append("Kh√¥ng t√¨m th·∫•y th·∫ª <span> trong c√πng trong th·∫ª <p> ngay sau th·∫ª ch·ª©a 'M√É NG√ÄNH'.")

    # X·ª≠ l√Ω n·ªôi dung ch√≠nh + ƒëi·ªÅu ki·ªán tuy·ªÉn sinh
    main_content = []
    extra_content = []
    found_condition = False

    for p in soup.select('p.MsoNormal'):
        text = p.get_text(strip=True)
        if not text:
            continue
        main_content.append(text)

        if "3. ƒêI·ªÄU KI·ªÜN TUY·ªÇN SINH" in text.upper() and not found_condition:
            found_condition = True
            tr_current = p.find_parent('tr')
            if tr_current:
                tr_next = tr_current.find_next_sibling('tr')
                if tr_next:
                    p_tags = tr_next.find_all('p')
                    for p_tag in p_tags:
                        spans = p_tag.find_all('span')
                        if spans:
                            for span in spans:
                                span_text = span.get_text(strip=True)
                                if span_text:
                                    extra_content.append(span_text)
                        else:
                            p_text = remove_emojis(p_tag.get_text(strip=True))
                            if p_text:
                                extra_content.append(p_text)

    for line in main_content:
        output_lines.append(remove_emojis(line))
        if "3. ƒêI·ªÄU KI·ªÜN TUY·ªÇN SINH" in line.upper() and extra_content:
            output_lines.extend(extra_content)

    return {
        "url": url,
        "ma_nganh": ma_nganh,
        "title": title if title_tag else None,
        "noidung": "\n".join(output_lines)
    }

def slugify(text):
    """Chuy·ªÉn m√£ ng√†nh ho·∫∑c t√™n ng√†nh th√†nh t√™n file h·ª£p l·ªá"""
    return re.sub(r'[^a-zA-Z0-9_-]', '_', text).lower()

def crawl_all(urls, output_dir="data/output_txt"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for idx, url in enumerate(urls):
        try:
            data = crawl_page(url)
            # T·∫°o t√™n file: m√£ ng√†nh ho·∫∑c s·ªë th·ª© t·ª±
            if data["title"]:
                filename = f"{slugify(remove_accents(data['title']))}.txt"
            else:
                filename = f"nganh_{idx + 1}.txt"
            path = os.path.join(output_dir, filename)

            with open(path, "w", encoding="utf-8") as f:
                f.write(f"URL: {data['url']}\n")
                if data["ma_nganh"]:
                    f.write(f"M√£ ng√†nh: {data['ma_nganh']}\n\n")
                f.write(data["noidung"])

            print(f"‚úÖ ƒê√£ l∆∞u: {filename}")
        except Exception as e:
            print(f"[!] L·ªói khi crawl {url}: {e}")

if __name__ == "__main__":
    danh_sach_url = [
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-thong-tin",
        "https://ts.huit.edu.vn/nganh-dh/nganh-marketing",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-ke-toan",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-ky-thuat-co-dien-tu",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-dam-bao-chat-luong-va-an-toan-thuc-pham",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-tai-chinh-ngan-hang",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-sinh-hoc",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-ngon-ngu-anh",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-luat-kinh-te",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-logistic-va-quan-ly-chuoi-cung-ung",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-thuong-mai-dien-tu",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-vat-lieu",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-luat.",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-kinh-doanh-quoc-te",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-ky-thuat-hoa-hoc",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-quan-ly-tai-nguyen-va-moi-truong",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-ky-thuat-co-dien-tu",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-quan-tri-khach-san",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-quan-tri-kinh-doanh-thuc-pham",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-det-may",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-quan-tri-kinh-doanh",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-luat-kinh-te",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-quan-tri-dich-vu-du-lich-va-lu-hanh",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-ky-thuat-dieu-khien-va-tu-dong-hoa",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-dam-bao-chat-luong-va-an-toan-thuc-pham",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-tai-chinh-ngan-hang",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-sinh-hoc",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-ngon-ngu-anh",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-cong-nghe-tai-chinh-fintech",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-an-toan-thong-tin",
        # "https://ts.huit.edu.vn/nganh-dh/nganh-khoa-hoc-dinh-duong-va-am-thuc"
    ]
    crawl_all(danh_sach_url)

# # ‚úÖ Ghi to√†n b·ªô output v√†o file
# with open("che-tao-may.txt", "w", encoding="utf-8") as f:
#     for line in output_lines:
#         f.write(line + "\n")

# # ‚úÖ In ra m√†n h√¨nh n·∫øu mu·ªën ki·ªÉm tra nhanh
# for line in output_lines:
#     print(line)
